# Resonance è®­ç»ƒå‚æ•°æŒ‡å—

æœ¬ç»¼åˆæŒ‡å—æ¶µç›–äº† **Resonance**ï¼ˆåŸResonanceï¼‰ä¸­æ‰€æœ‰å¯ç”¨çš„è®­ç»ƒå‚æ•°ã€‚ResonanceåŸºäº [Transformers](https://github.com/huggingface/transformers) æ„å»ºï¼Œå› æ­¤æ‰€æœ‰æ ‡å‡†Transformersè®­ç»ƒå‚æ•°ä¹Ÿå¯ç”¨ã€‚

## ï¿½ï¿½ ç›®å½•

- [é€šç”¨å‚æ•°](#é€šç”¨å‚æ•°)
- [æ¨¡å‹ä¸æ¶æ„](#æ¨¡å‹ä¸æ¶æ„)
- [æ•°æ®é›†é…ç½®](#æ•°æ®é›†é…ç½®)
- [LoRAä¸é«˜æ•ˆè®­ç»ƒ](#loraä¸é«˜æ•ˆè®­ç»ƒ)
- [è®­ç»ƒè¶…å‚æ•°](#è®­ç»ƒè¶…å‚æ•°)
- [DPOç‰¹å®šå‚æ•°](#dpoç‰¹å®šå‚æ•°)
- [PPOç‰¹å®šå‚æ•°](#ppoç‰¹å®šå‚æ•°)
- [è¯„ä¼°ä¸æ—¥å¿—](#è¯„ä¼°ä¸æ—¥å¿—)
- [é«˜çº§é€‰é¡¹](#é«˜çº§é€‰é¡¹)

---

## é€šç”¨å‚æ•°

è¿™äº›å‚æ•°åœ¨æ‰€æœ‰Resonanceè®­ç»ƒå™¨ï¼ˆSFTã€DPOã€PPOç­‰ï¼‰ä¸­å…±äº«ï¼š

### æ¨¡å‹ä¸æ¶æ„

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--model_name_or_path` | str | **å¿…éœ€** | é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ–HuggingFace Hub ID |
| `--freeze_vision_tower` | bool | `True` | è®­ç»ƒæœŸé—´æ˜¯å¦å†»ç»“è§†è§‰ç¼–ç å™¨ |
| `--use_flash_attention_2` | bool | `False` | å¯ç”¨FlashAttention-2è¿›è¡Œå†…å­˜é«˜æ•ˆè®­ç»ƒ |

### æ•°æ®é›†é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--dataset_name` | str | **å¿…éœ€** | æ•°æ®é›†ç±»å‹æ ‡è¯†ç¬¦ï¼ˆå‚è§[æ•°æ®é›†ç±»å‹](#æ•°æ®é›†ç±»å‹)ï¼‰ |
| `--data_path` | str | `None` | è‡ªå®šä¹‰æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„ |
| `--image_root` | str | `None` | å›¾åƒæ ¹ç›®å½•ï¼ˆç”¨äºè‡ªå®šä¹‰æ•°æ®é›†ï¼‰ |
| `--data_ratio` | float | `1.0` | ç”¨äºè®­ç»ƒçš„æ•°æ®æ¯”ä¾‹ |
| `--dataset_num_proc` | int | `4` | æ•°æ®é¢„å¤„ç†çš„è¿›ç¨‹æ•° |

#### æ•°æ®é›†ç±»å‹

| `dataset_name` | æè¿° | ä½¿ç”¨åœºæ™¯ |
|----------------|------|----------|
| `vlfeedback_paired` | [VLFeedback](https://huggingface.co/datasets/MMInstruction/VLFeedback) æ•°æ®é›† | å¸¦åå¥½å¯¹çš„DPOè®­ç»ƒ |
| `rlhfv` | [RLHF-V](https://huggingface.co/datasets/HaoyeZhang/RLHF-V-Dataset) æ•°æ®é›† | RLHFè®­ç»ƒ |
| `vlquery_json` | JSONæ ¼å¼çš„è‡ªå®šä¹‰å¯¹è¯æ•°æ® | SFTè®­ç»ƒ |
| `plain_dpo` | JSONæ ¼å¼çš„è‡ªå®šä¹‰æ¯”è¾ƒæ•°æ® | DPOè®­ç»ƒ |

### é•¿åº¦é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--max_length` | int | `2048` | æ•´ä¸ªè¾“å…¥çš„æœ€å¤§åºåˆ—é•¿åº¦ |
| `--max_prompt_length` | int | `1024` | æç¤ºéƒ¨åˆ†çš„æœ€å¤§é•¿åº¦ |
| `--max_target_length` | int | `1024` | ç›®æ ‡/å“åº”éƒ¨åˆ†çš„æœ€å¤§é•¿åº¦ |

---

## LoRAä¸é«˜æ•ˆè®­ç»ƒ

Resonanceæ”¯æŒä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œé‡åŒ–LoRAï¼ˆQLoRAï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆè®­ç»ƒï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--use_lora` | bool | `False` | å¯ç”¨LoRAå¾®è°ƒ |
| `--lora_r` | int | `64` | LoRAç§©ï¼ˆè¶Šé«˜=æ›´å¤šå‚æ•°ï¼Œæ›´å¥½è´¨é‡ï¼‰ |
| `--lora_alpha` | int | `16` | LoRAç¼©æ”¾å‚æ•° |
| `--lora_dropout` | float | `0.05` | LoRA dropoutç‡ |
| `--lora_target_modules` | str | `"auto"` | LoRAç›®æ ‡æ¨¡å—ï¼ˆé€—å·åˆ†éš”æˆ–"auto"ï¼‰ |
| `--lora_bias` | str | `"none"` | LoRAåç½®ç­–ç•¥ï¼š"none"ã€"all"æˆ–"lora_only" |
| `--q_lora` | bool | `False` | å¯ç”¨QLoRAï¼ˆé‡åŒ–LoRAï¼‰ |
| `--bits` | int | `4` | QLoRAé‡åŒ–ä½æ•°ï¼ˆ4æˆ–8ï¼‰ |
| `--modules_to_save` | str | `None` | è¦ä¿å­˜çš„é¢å¤–æ¨¡å—ï¼ˆé€—å·åˆ†éš”ï¼‰ |

### LoRAç›®æ ‡æ¨¡å—ç¤ºä¾‹

```bash
# ä»…ç”¨äºæ³¨æ„åŠ›å±‚
--lora_target_modules "c_attn,attn.c_proj"

# ç”¨äºæ³¨æ„åŠ›å’ŒMLPå±‚
--lora_target_modules "c_attn,attn.c_proj,w1,w2"

# è®©Resonanceè‡ªåŠ¨æ£€æµ‹æœ€ä¼˜ç›®æ ‡
--lora_target_modules "auto"
```

---

## è®­ç»ƒè¶…å‚æ•°

æ¥è‡ªTransformersçš„æ ‡å‡†è®­ç»ƒå‚æ•°ã€‚å…³é”®å»ºè®®ï¼š

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--per_device_train_batch_size` | `2-8` | æ ¹æ®GPUå†…å­˜è°ƒæ•´ |
| `--gradient_accumulation_steps` | `4-16` | å¢åŠ ä»¥è·å¾—æ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å° |
| `--learning_rate` | `1e-5` åˆ° `5e-5` | è¾ƒä½å€¼æ›´ç¨³å®š |
| `--weight_decay` | `0.01-0.05` | æ­£åˆ™åŒ– |
| `--warmup_ratio` | `0.1` | 10%é¢„çƒ­æ­¥æ•° |
| `--lr_scheduler_type` | `"cosine"` | å­¦ä¹ ç‡è¡°å‡ |
| `--num_train_epochs` | `1-3` | é€šå¸¸å‡ ä¸ªepochå°±è¶³å¤Ÿ |
| `--bf16` | `True` | å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨bfloat16 |
| `--gradient_checkpointing` | `True` | ä»¥é€Ÿåº¦ä¸ºä»£ä»·èŠ‚çœå†…å­˜ |

---

## DPOç‰¹å®šå‚æ•°

ç”¨äºç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--beta` | float | `0.1` | DPOæ¸©åº¦å‚æ•°ï¼ˆè¶Šé«˜=è¶Šä¿å®ˆï¼‰ |
| `--score_margin` | float | `0.0` | åå¥½åˆ†æ•°è¾¹è· |

---

## PPOç‰¹å®šå‚æ•°

ç”¨äºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è®­ç»ƒï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--kl_coef` | float | `0.1` | KLæ•£åº¦ç³»æ•° |
| `--cliprange` | float | `0.2` | PPOè£å‰ªèŒƒå›´ |
| `--vf_coef` | float | `0.5` | ä»·å€¼å‡½æ•°ç³»æ•° |

---

## è¯„ä¼°ä¸æ—¥å¿—

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `--eval_strategy` | str | `"steps"` | è¯„ä¼°ç­–ç•¥ï¼š"no"ã€"steps"ã€"epoch" |
| `--eval_steps` | int | `500` | æ¯Næ­¥è¯„ä¼°ä¸€æ¬¡ |
| `--save_strategy` | str | `"steps"` | ä¿å­˜ç­–ç•¥ï¼š"no"ã€"steps"ã€"epoch" |
| `--save_steps` | int | `1000` | æ¯Næ­¥ä¿å­˜ä¸€æ¬¡ |
| `--save_total_limit` | int | `3` | ä¿ç•™çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•° |
| `--logging_steps` | int | `50` | æ¯Næ­¥è®°å½•ä¸€æ¬¡ |
| `--report_to` | str | `"wandb"` | æ—¥å¿—æœåŠ¡ï¼š"wandb"ã€"tensorboard"ã€"none" |
| `--project_name` | str | `"Resonance"` | æ—¥å¿—é¡¹ç›®åç§° |
| `--group_name` | str | `None` | å®éªŒç»„åç§° |
| `--run_name` | str | `None` | ç‰¹å®šè¿è¡Œåç§° |

---

## é«˜çº§é€‰é¡¹

### å†…å­˜ä¼˜åŒ–

```bash
# ç”¨äºæœ‰é™å†…å­˜ä¸Šçš„å¤§å‹æ¨¡å‹
--gradient_checkpointing True \
--use_lora True \
--q_lora True \
--bits 4 \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 16
```

### å¤šGPUè®­ç»ƒ

```bash
# ä¸accelerateä¸€èµ·ä½¿ç”¨
accelerate launch --config_file accelerate_config/zero2.yaml \
    --num_processes 4 \
    src/resonance/dpo.py \
    # ... å…¶ä»–å‚æ•°
```

### è‡ªå®šä¹‰æ•°æ®é›†

å¯¹äºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å‡†å¤‡JSONæ–‡ä»¶ï¼š

**SFTæ•°æ®é›†ï¼ˆ`vlquery_json`ï¼‰ï¼š**
```json
[
  {
    "image": "path/to/image.jpg",
    "conversations": [
      {"from": "user", "value": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
      {"from": "assistant", "value": "æˆ‘å¯ä»¥çœ‹åˆ°..."}
    ]
  }
]
```

**DPOæ•°æ®é›†ï¼ˆ`plain_dpo`ï¼‰ï¼š**
```json
[
  {
    "image": "path/to/image.jpg",
    "prompt": "æè¿°è¿™å¼ å›¾ç‰‡",
    "chosen": "é«˜è´¨é‡å›ç­”",
    "rejected": "ä½è´¨é‡å›ç­”"
  }
]
```

---

## é…ç½®ç¤ºä¾‹

### å¿«é€Ÿæ¼”ç¤ºè®­ç»ƒ
```bash
python src/resonance/sft.py \
    --model_name_or_path microsoft/DialoGPT-medium \
    --dataset_name vlquery_json \
    --data_path demos/data/demo_sft.json \
    --image_root demos/images \
    --max_length 512 \
    --per_device_train_batch_size 2 \
    --num_train_epochs 1 \
    --learning_rate 5e-5 \
    --use_lora True \
    --lora_r 16 \
    --report_to none
```

### ç”Ÿäº§DPOè®­ç»ƒ
```bash
accelerate launch --config_file accelerate_config/zero2.yaml \
    src/resonance/dpo.py \
    --model_name_or_path Qwen/Qwen-VL-Chat \
    --dataset_name vlfeedback_paired \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-5 \
    --num_train_epochs 1 \
    --use_lora True \
    --lora_r 64 \
    --lora_alpha 16 \
    --beta 0.1 \
    --max_length 1024 \
    --bf16 True \
    --gradient_checkpointing True \
    --project_name "Resonance" \
    --group_name "Qwen-VL-DPO"
```

æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ [demos](../demos/) ç›®å½•ï¼

## å‚æ•°è°ƒä¼˜å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–ç­–ç•¥
- **å°å†…å­˜GPU**ï¼šä½¿ç”¨QLoRA + æ¢¯åº¦æ£€æŸ¥ç‚¹
- **ä¸­ç­‰å†…å­˜GPU**ï¼šä½¿ç”¨LoRA + æ··åˆç²¾åº¦
- **å¤§å†…å­˜GPU**ï¼šå…¨å‚æ•°å¾®è°ƒ + å¤§æ‰¹æ¬¡

### 2. å­¦ä¹ ç‡é€‰æ‹©
- **SFTè®­ç»ƒ**ï¼š1e-5 åˆ° 5e-5
- **DPOè®­ç»ƒ**ï¼š1e-5 åˆ° 2e-5ï¼ˆæ›´ä¿å®ˆï¼‰
- **PPOè®­ç»ƒ**ï¼š5e-6 åˆ° 1e-5ï¼ˆæœ€ä¿å®ˆï¼‰

### 3. æ‰¹æ¬¡å¤§å°é…ç½®
- **å•GPU**ï¼šper_device_train_batch_size = 2-4
- **å¤šGPU**ï¼šper_device_train_batch_size = 1-2
- **æ¢¯åº¦ç´¯ç§¯**ï¼šgradient_accumulation_steps = 4-16

### 4. è®­ç»ƒè½®æ•°å»ºè®®
- **SFT**ï¼š1-3ä¸ªepoch
- **DPO**ï¼š1-2ä¸ªepoch
- **PPO**ï¼š1ä¸ªepochï¼ˆé€šå¸¸è¶³å¤Ÿï¼‰

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©LoRAå‚æ•°ï¼Ÿ
A: ä»r=16å¼€å§‹ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ã€‚å¤æ‚ä»»åŠ¡ä½¿ç”¨r=64ï¼Œç®€å•ä»»åŠ¡ä½¿ç”¨r=16ã€‚

### Q: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨QLoRAï¼Ÿ
A: å½“GPUå†…å­˜ä¸è¶³æ—¶ï¼ŒQLoRAå¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä½†å¯èƒ½ç•¥å¾®å½±å“æ€§èƒ½ã€‚

### Q: å¦‚ä½•ç¡®å®šå­¦ä¹ ç‡ï¼Ÿ
A: ä»1e-5å¼€å§‹ï¼Œå¦‚æœè®­ç»ƒä¸ç¨³å®šåˆ™é™ä½ï¼Œå¦‚æœæ”¶æ•›å¤ªæ…¢åˆ™é€‚å½“æé«˜ã€‚

### Q: æ‰¹æ¬¡å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ
A: åœ¨å†…å­˜å…è®¸çš„æƒ…å†µä¸‹å°½å¯èƒ½å¤§ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ã€‚


# Resonance Training Arguments Guide

This comprehensive guide covers all training arguments available in **Resonance** (formerly Resonance). Resonance builds on [Transformers](https://github.com/huggingface/transformers), so all standard Transformers training arguments are also available.

## ğŸ“‹ Table of Contents

- [Common Arguments](#common-arguments)
- [Model & Architecture](#model--architecture)
- [Dataset Configuration](#dataset-configuration)  
- [LoRA & Efficient Training](#lora--efficient-training)
- [Training Hyperparameters](#training-hyperparameters)
- [DPO-Specific Arguments](#dpo-specific-arguments)
- [PPO-Specific Arguments](#ppo-specific-arguments)
- [Evaluation & Logging](#evaluation--logging)
- [Advanced Options](#advanced-options)

---

## Common Arguments

These arguments are shared across all Resonance trainers (SFT, DPO, PPO, etc.):

### Model & Architecture

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--model_name_or_path` | str | **Required** | Path or HuggingFace Hub ID of the pretrained model |
| `--freeze_vision_tower` | bool | `True` | Whether to freeze the vision encoder during training |
| `--use_flash_attention_2` | bool | `False` | Enable FlashAttention-2 for memory-efficient training |

### Dataset Configuration

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--dataset_name` | str | **Required** | Dataset type identifier (see [Dataset Types](#dataset-types)) |
| `--data_path` | str | `None` | Path to custom dataset JSON file |
| `--image_root` | str | `None` | Root directory for images (for custom datasets) |
| `--data_ratio` | float | `1.0` | Ratio of data to use for training |
| `--dataset_num_proc` | int | `4` | Number of processes for data preprocessing |

#### Dataset Types

| `dataset_name` | Description | Use Case |
|----------------|-------------|----------|
| `vlfeedback_paired` | [VLFeedback](https://huggingface.co/datasets/MMInstruction/VLFeedback) dataset | DPO training with preference pairs |
| `rlhfv` | [RLHF-V](https://huggingface.co/datasets/HaoyeZhang/RLHF-V-Dataset) dataset | RLHF training |
| `vlquery_json` | Custom conversation data in JSON format | SFT training |
| `plain_dpo` | Custom comparison data in JSON format | DPO training |

### Length Configuration

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--max_length` | int | `2048` | Maximum sequence length for the entire input |
| `--max_prompt_length` | int | `1024` | Maximum length for the prompt portion |
| `--max_target_length` | int | `1024` | Maximum length for the target/response portion |

---

## LoRA & Efficient Training

Resonance supports Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) for parameter-efficient training:

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--use_lora` | bool | `False` | Enable LoRA fine-tuning |
| `--lora_r` | int | `64` | LoRA rank (higher = more parameters, better quality) |
| `--lora_alpha` | int | `16` | LoRA scaling parameter |
| `--lora_dropout` | float | `0.05` | LoRA dropout rate |
| `--lora_target_modules` | str | `"auto"` | Target modules for LoRA (comma-separated or "auto") |
| `--lora_bias` | str | `"none"` | LoRA bias strategy: "none", "all", or "lora_only" |
| `--q_lora` | bool | `False` | Enable QLoRA (quantized LoRA) |
| `--bits` | int | `4` | Quantization bits for QLoRA (4 or 8) |
| `--modules_to_save` | str | `None` | Additional modules to save (comma-separated) |

### LoRA Target Modules Examples

```bash
# For attention layers only
--lora_target_modules "c_attn,attn.c_proj"

# For attention and MLP layers  
--lora_target_modules "c_attn,attn.c_proj,w1,w2"

# Let Resonance auto-detect optimal targets
--lora_target_modules "auto"
```

---

## Training Hyperparameters

Standard training arguments from Transformers. Key recommendations:

| Argument | Recommended | Notes |
|----------|-------------|-------|
| `--per_device_train_batch_size` | `2-8` | Adjust based on GPU memory |
| `--gradient_accumulation_steps` | `4-16` | Increase for larger effective batch size |
| `--learning_rate` | `1e-5` to `5e-5` | Lower for stability |
| `--weight_decay` | `0.01-0.05` | Regularization |
| `--warmup_ratio` | `0.1` | 10% warmup steps |
| `--lr_scheduler_type` | `"cosine"` | Learning rate decay |
| `--num_train_epochs` | `1-3` | Few epochs usually sufficient |
| `--bf16` | `True` | Use bfloat16 if supported |
| `--gradient_checkpointing` | `True` | Save memory at cost of speed |

---

## DPO-Specific Arguments

For Direct Preference Optimization training:

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--beta` | float | `0.1` | DPO temperature parameter (higher = more conservative) |
| `--score_margin` | float | `0.0` | Margin for preference scores |

---

## PPO-Specific Arguments

For Proximal Policy Optimization training:

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--kl_coef` | float | `0.1` | KL divergence coefficient |
| `--cliprange` | float | `0.2` | PPO clipping range |
| `--vf_coef` | float | `0.5` | Value function coefficient |

---

## Evaluation & Logging

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--eval_strategy` | str | `"steps"` | Evaluation strategy: "no", "steps", "epoch" |
| `--eval_steps` | int | `500` | Evaluate every N steps |
| `--save_strategy` | str | `"steps"` | Save strategy: "no", "steps", "epoch" |  
| `--save_steps` | int | `1000` | Save every N steps |
| `--save_total_limit` | int | `3` | Maximum number of checkpoints to keep |
| `--logging_steps` | int | `50` | Log every N steps |
| `--report_to` | str | `"wandb"` | Logging service: "wandb", "tensorboard", "none" |
| `--project_name` | str | `"Resonance"` | Project name for logging |
| `--group_name` | str | `None` | Experiment group name |
| `--run_name` | str | `None` | Specific run name |

---

## Advanced Options

### Memory Optimization

```bash
# For large models on limited memory
--gradient_checkpointing True \
--use_lora True \
--q_lora True \
--bits 4 \
--per_device_train_batch_size 1 \
--gradient_accumulation_steps 16
```

### Multi-GPU Training

```bash
# Use with accelerate
accelerate launch --config_file accelerate_config/zero2.yaml \
    --num_processes 4 \
    src/resonance/dpo.py \
    # ... other arguments
```

### Custom Datasets

For custom datasets, prepare JSON files following these formats:

**SFT Dataset (`vlquery_json`):**
```json
[
  {
    "image": "path/to/image.jpg",
    "conversations": [
      {"from": "user", "value": "What's in this image?"},
      {"from": "assistant", "value": "I can see..."}
    ]
  }
]
```

**DPO Dataset (`plain_dpo`):**
```json
[
  {
    "image": "path/to/image.jpg", 
    "prompt": "Describe this image",
    "chosen": "High quality response",
    "rejected": "Low quality response"
  }
]
```

---

## Example Configurations

### Quick Demo Training
```bash
python src/resonance/sft.py \
    --model_name_or_path microsoft/DialoGPT-medium \
    --dataset_name vlquery_json \
    --data_path demos/data/demo_sft.json \
    --image_root demos/images \
    --max_length 512 \
    --per_device_train_batch_size 2 \
    --num_train_epochs 1 \
    --learning_rate 5e-5 \
    --use_lora True \
    --lora_r 16 \
    --report_to none
```

### Production DPO Training  
```bash
accelerate launch --config_file accelerate_config/zero2.yaml \
    src/resonance/dpo.py \
    --model_name_or_path Qwen/Qwen-VL-Chat \
    --dataset_name vlfeedback_paired \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-5 \
    --num_train_epochs 1 \
    --use_lora True \
    --lora_r 64 \
    --lora_alpha 16 \
    --beta 0.1 \
    --max_length 1024 \
    --bf16 True \
    --gradient_checkpointing True \
    --project_name "Resonance" \
    --group_name "Qwen-VL-DPO"
```

For more examples, check out the [demos](../demos/) directory!
